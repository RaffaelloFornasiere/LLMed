name: llmed

networks:
  hbd-demo-network:
volumes:
  elasticsearch-data:
    driver: local

services:
  nginx:
    image: nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "9090:8080"
    networks:
      - hbd-demo-network
    extra_hosts:
      - "host.docker.internal:host-gateway"  #
    depends_on:
      hbd_frontend:
        condition: service_started
      hbd_backend:
        condition: service_started



#  hbd_llamacpp:
#    container_name: hbd_llamacpp
#    image: llama-cpp-python:latest
#    build:
#      context: ./llama-server/
#      dockerfile: Dockerfile
#    volumes:
#      - ~/models/:/models/
#    command: sh -c "python3 -m llama_cpp.server --model /models/Meta-Llama-3-8B-Instruct-Q8_0.gguf --n_ctx 8182 --cache true --n_gpu_layers -1"
#    environment:
#      - HOST=0.0.0.0
#    deploy:
#      resources:
#        limits:
#          memory: 32GB
#        reservations:
#          devices:
#            - driver: nvidia
#              device_ids: ["0"]
#              capabilities: [ gpu ]
#    restart: "always"
#    networks:
#      - hbd-demo-network

  hbd_frontend:
    image: hbd-demo-frontend:latest
    build:
      context: ./frontend/
      dockerfile: Dockerfile
    volumes:
      - ./frontend/:/usr/src/app/:cached
    # npm install
    container_name: hbd_frontend
    environment:
      - CHOKIDAR_USEPOLLING=true
      - NODE_ENV=development
    restart: "always"
    command: sh -c "npm install && npm audit fix --force && quasar dev --watch"
    networks:
      - hbd-demo-network

  hbd_backend:
    build:
      context: ./backend/
      dockerfile: Dockerfile
    image: hbd-demo-backend:latest
    container_name: hbd_backend
    volumes:
      - ./backend/:/workspace/
      - ~/models/:/models/
    environment:
      - PYTHONUNBUFFERED=1
      - GROQ_API_KEY=${GROQ_API_KEY}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: "always"
    networks:
      - hbd-demo-network



